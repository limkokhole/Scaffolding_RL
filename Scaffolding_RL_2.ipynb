{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scaffolding_RL_2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/serenabooth/Scaffolding_RL/blob/master/Scaffolding_RL_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "0jKalEUf2b0j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Credit: https://cgnicholls.github.io/reinforcement-learning/2017/03/27/a3c.html"
      ]
    },
    {
      "metadata": {
        "id": "vCWQCQ5y2geR",
        "colab_type": "code",
        "outputId": "beaf1e64-f3c2-438f-9097-824cdacd1a46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1495
        }
      },
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install -y python-numpy python-dev cmake zlib1g-dev libjpeg-dev xvfb ffmpeg xorg-dev python-opengl libboost-all-dev libsdl2-dev swig\n",
        "!apt-get install python-opengl -y\n",
        "!apt install xvfb -y\n",
        "!pip install gym\n",
        "!pip install \"gym[atari]\"\n",
        "!pip install tqdm\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install piglet"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com] [Wait\r0% [1 InRelease gpgv 3,609 B] [Connecting to archive.ubuntu.com] [Connecting to\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [1 InRelease gpgv 3,609 B] [Connecting to archive.ubuntu.com] [Connecting to\r0% [Connecting to archive.ubuntu.com (91.189.88.162)] [Connecting to security.u\r                                                                               \rIgn:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.162)] [Connecting to security.u\r                                                                               \rHit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:7 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:9 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:12 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Fetched 252 kB in 3s (100 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "cmake is already the newest version (3.10.2-1ubuntu2).\n",
            "libjpeg-dev is already the newest version (8c-2ubuntu8).\n",
            "python-dev is already the newest version (2.7.15~rc1-1).\n",
            "python-numpy is already the newest version (1:1.13.3-2ubuntu1).\n",
            "zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).\n",
            "libboost-all-dev is already the newest version (1.65.1.0ubuntu1).\n",
            "python-opengl is already the newest version (3.1.0+dfsg-1).\n",
            "swig is already the newest version (3.0.12-1).\n",
            "xorg-dev is already the newest version (1:7.7+19ubuntu7.1).\n",
            "ffmpeg is already the newest version (7:3.4.4-0ubuntu0.18.04.1).\n",
            "libsdl2-dev is already the newest version (2.0.8+dfsg1-1ubuntu1.18.04.3).\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.2).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 44 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python-opengl is already the newest version (3.1.0+dfsg-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 44 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.2).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 44 not upgraded.\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.10.11)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.2.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.16.3)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (2.21.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.12.0)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2019.3.9)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (1.24.2)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym) (0.16.0)\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.6/dist-packages (0.10.11)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.2.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.16.3)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (2.21.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.12.0)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.3.2)\n",
            "Requirement already satisfied: atari_py>=0.1.4 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (0.1.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (4.3.0)\n",
            "Requirement already satisfied: PyOpenGL in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (3.1.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[atari]) (1.24.2)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[atari]) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[atari]) (2019.3.9)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[atari]) (2.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym[atari]) (0.16.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow->gym[atari]) (0.46)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.6/dist-packages (0.2.1)\n",
            "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.6/dist-packages (from pyvirtualdisplay) (0.2.5)\n",
            "Requirement already satisfied: piglet in /usr/local/lib/python3.6/dist-packages (0.4.4)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.6/dist-packages (from piglet) (1.1.1)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.6/dist-packages (from piglet) (1.6.2)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from piglet) (19.1.0)\n",
            "Requirement already satisfied: Parsley in /usr/local/lib/python3.6/dist-packages (from piglet) (1.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet) (0.33.1)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vrTqGPjB2kfW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pyvirtualdisplay import Display\n",
        "from IPython.core import display\n",
        "from scipy.misc import imresize\n",
        "import gym\n",
        "from gym.core import Wrapper\n",
        "from gym.spaces.box import Box\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import sys, getopt\n",
        "import threading\n",
        "import numpy as np\n",
        "from time import time, sleep\n",
        "import queue\n",
        "import random\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Rh_ca0JKz8Vh",
        "colab_type": "code",
        "outputId": "e29da409-f7b9-4c51-a244-5d11929700a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0l4otSn5h8y_",
        "colab_type": "code",
        "outputId": "197e8907-5745-4816-929e-f47edf55f64d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "cell_type": "code",
      "source": [
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1001'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '1400x900x24', ':1001'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "qNWUrczaibv4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#If you are running on a server, launch xvfb to record game videos\n",
        "#Please make sure you have xvfb installed\n",
        "if os.environ.get(\"DISPLAY\") is str and len(os.environ.get(\"DISPLAY\"))!=0:\n",
        "    !bash ../xvfb start\n",
        "    %env DISPLAY=:1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u2mBTud820d9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Train for this many steps\n",
        "T_MAX = 100000000\n",
        "# Use this many threads\n",
        "NUM_THREADS = 8\n",
        "# Initial learning rate for Adam\n",
        "INITIAL_LEARNING_RATE = 1e-4\n",
        "# The discount factor\n",
        "DISCOUNT_FACTOR = 0.99\n",
        "# Evaluate the agent and print out average reward every this many steps\n",
        "VERBOSE_EVERY = 40000\n",
        "# Update the parameters in each thread after this many steps in that thread\n",
        "I_ASYNC_UPDATE = 5\n",
        "TESTING = False\n",
        "\n",
        "\n",
        "FLAGS = {\"T_MAX\": T_MAX, \"NUM_THREADS\": NUM_THREADS, \"INITIAL_LEARNING_RATE\":\n",
        "INITIAL_LEARNING_RATE, \"DISCOUNT_FACTOR\": DISCOUNT_FACTOR, \"VERBOSE_EVERY\":\n",
        "VERBOSE_EVERY, \"TESTING\": TESTING, \"I_ASYNC_UPDATE\": I_ASYNC_UPDATE}\n",
        "\n",
        "# Use this global variable to exit the training loop in each thread once we've finished.\n",
        "training_finished = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MM7dJM8C266-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "env = gym.make('SpaceInvaders-v0')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "voukReYU3CCT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# A wrapper class for Open AI Gym.\n",
        "class CustomGym(Wrapper):\n",
        "    def __init__(self, env, game_name, skip_actions=4, num_frames=4, w=84, h=84, dim_order='theano', color=False, crop=lambda img: img):\n",
        "      # game_name: the name of the Open AI Gym environment\n",
        "      # skip_actions: the number of frames to repeat an action for\n",
        "      # num_frames: the number of frames to stack in one state\n",
        "      # w: width of the state input\n",
        "      # h: height of the state input.\n",
        "      super(CustomGym, self).__init__(env)\n",
        "\n",
        "      #self.env = gym.make(game_name)\n",
        "      self.crop = crop\n",
        "      self.num_frames = num_frames\n",
        "      self.skip_actions = skip_actions\n",
        "      self.w = w\n",
        "      self.h = h\n",
        "      self.dim_order = dim_order\n",
        "      self.color = color\n",
        "      \n",
        "      n_channels = (3 * num_frames) if color else num_frames\n",
        "      obs_shape = [n_channels,h,w] if dim_order == 'theano' else [h,w,n_channels]\n",
        "      self.observation_space = Box(0.0, 1.0, obs_shape)\n",
        "      self.framebuffer = np.zeros(obs_shape, 'float32')\n",
        "\n",
        "#       # For some of the ATARI games the action spaces are much larger than we want\n",
        "#       # and contain repeated actions, so I worked out simplified actions for these\n",
        "#       # games.\n",
        "#       if game_name == 'SpaceInvaders-v0':\n",
        "#         self.action_space = [1,2,3]\n",
        "#       elif game_name == 'Pong-v0':\n",
        "#         self.action_space = [1,2,3]\n",
        "#       elif game_name == 'Breakout-v0':\n",
        "#         self.action_space = [1,4,5]\n",
        "#       else:\n",
        "#         # Otherwise, use the actions specified by Open AI.\n",
        "#         self.action_space = env.action_space\n",
        "\n",
        "      # Store the rest.\n",
        "      self.action_size = self.action_space.n\n",
        "      self.game_name = game_name\n",
        "    \n",
        "    def update_buffer(self,img):\n",
        "        img = self.preproc_image(img)\n",
        "        offset = 3 if self.color else 1\n",
        "        if self.dim_order == 'theano':\n",
        "            axis = 0\n",
        "            cropped_framebuffer = self.framebuffer[:-offset]\n",
        "        else:\n",
        "            axis = -1\n",
        "            cropped_framebuffer = self.framebuffer[:,:,:-offset]\n",
        "        self.framebuffer = np.concatenate([img, cropped_framebuffer], axis = axis)\n",
        "\n",
        "    def preproc_image(self, img):\n",
        "        \"\"\"what happens to the observation\"\"\"\n",
        "        img = self.crop(img)\n",
        "        #img = imresize(img, self.img_size)\n",
        "        img = np.array(Image.fromarray(img).resize((self.w, self.h)))\n",
        "        if not self.color:\n",
        "            img = img.mean(-1, keepdims=True)\n",
        "        if self.dim_order == 'theano':\n",
        "            img = img.transpose([2,0,1]) # [h, w, c] to [c, h, w]\n",
        "        img = img.astype('float32') / 255.\n",
        "        return img\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"resets breakout, returns initial frames\"\"\"\n",
        "        self.framebuffer = np.zeros_like(self.framebuffer)\n",
        "        self.update_buffer(self.env.reset())\n",
        "        return self.framebuffer\n",
        "    \n",
        "    # Step the environment with the given action.\n",
        "    def step(self, action):\n",
        "      #action = self.action_space[action_idx]\n",
        "      accum_reward = 0\n",
        "      prev_s = None\n",
        "      for _ in range(self.skip_actions):\n",
        "        s, r, term, info = self.env.step(action)\n",
        "        accum_reward += r\n",
        "        if term:\n",
        "          break\n",
        "        prev_s = s\n",
        "      self.update_buffer(s)\n",
        "\n",
        "      # Takes maximum value for each pixel value over the current and previous\n",
        "      # frame. Used to get round Atari sprites flickering (Mnih et al. (2015))\n",
        "      if self.game_name == 'SpaceInvaders-v0' and prev_s is not None:\n",
        "        s = np.maximum.reduce([s, prev_s])\n",
        "      return self.framebuffer, accum_reward, term, info"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IWZkyMAcfw6p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "outputId": "d6dd9db0-3711-4bc3-f662-38f355827ffc"
      },
      "cell_type": "code",
      "source": [
        "def make_env(game_name):\n",
        "    env = gym.make(game_name)\n",
        "    env = CustomGym(env, game_name=game_name, h=42, w=42,\n",
        "                          crop = lambda img: img[35:-20, 20:],\n",
        "                          dim_order = 'tensorflow',\n",
        "                          color=False, num_frames=4)\n",
        "    return env\n",
        "\n",
        "env = make_env(\"SpaceInvaders-v0\")\n",
        "\n",
        "obs_shape = env.observation_space.shape\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "print(\"Observation shape:\", obs_shape)\n",
        "print(\"Num actions:\", n_actions)\n",
        "print(\"Action names:\", env.env.env.get_action_meanings())\n",
        "\n",
        "s = env.reset()\n",
        "for _ in range(100):\n",
        "    s, _, _, _ = env.step(env.action_space.sample())\n",
        "\n",
        "plt.title('Game image')\n",
        "plt.imshow(env.render('rgb_array'))\n",
        "plt.show()\n",
        "\n",
        "plt.title('Agent observation (4-frame buffer)')\n",
        "print (env.observation_space.shape)\n",
        "plt.imshow(s.transpose([0,2,1]).reshape([42,-1]))\n",
        "plt.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Observation shape: (42, 42, 4)\n",
            "Num actions: 6\n",
            "Action names: ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAANEAAAEICAYAAADBfBG8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAFXdJREFUeJzt3X/wHHV9x/Hni0TQAQoJYBpDIMCg\nFrSNEYEpEmlVQFAD2mJoq4i0kQptbXEkBMdSazOAosVRsVFpoYMglR9SFRWZqtNBfgTEEIhIgCDJ\nhAT5jT/QwLt/7J5srnff79199nb37vt6zOx877s/PvfeuX3f57Of2/2sIgIzG9w2dQdgNuqcRGaJ\nnERmiZxEZomcRGaJnERmiZxEY0jSHpKeljSt7limAidRAkmLJd0k6eeSNuev3ydJdcYVET+NiB0i\n4tk645gqnEQDknQacD7wMeB3gVnAycAhwLY1hmZViwhPfU7ATsDPgbdPst7RwA+BJ4EHgbMKy+YB\nAZyYL3uMLAlfA6wCHgc+3Vbee4A1+brfAvbs8r6tsqfn/38X+ChwA/A08N/ALsAleWy3APMK25+f\nx/QkcCtwaGHZi4CL8hjWAB8E1heWvwS4AngYuB/427o/r6EfD3UHMIoTcCSwpXWQTrDeYcAryWr8\n3wc2Acfky1oH+ueAFwKHA78CrgZeDMwBNgOvy9dfBKwFfg+YDnwIuKHL+3ZKorXAPvkXwF3AT4A3\n5GVdDPx7Yfu/yJNsOnAa8BDwwnzZ2cD3gBnA7nnCr8+XbZMn3YfJauO9gfuAI+r+zIZ6PNQdwChO\n+UH2UNu8G/La45fAwi7b/Svwyfx160CfU1j+CPCOwv9XAO/PX18LnFRYtg3wCzrURl2S6MzC8vOA\nawv/vwW4fYL9fQz4g/z1VkkB/GUhiQ4Cftq27RnFBB3HyedEg3kE2FXS9NaMiPjDiNg5X7YNgKSD\nJP2PpIclPUHWXNu1raxNhde/7PD/DvnrPYHzJT0u6XHgUUBkNVYven0fJH1A0hpJT+TvtVMh7peQ\nNfVaiq/3BF7SijHfdhnZ+eLYchIN5gfAM2RNrIl8CbgGmBsRO5E13QbtuXsQeG9E7FyYXhQRNwxY\nXkeSDiU7zzkOmJF/MTzB83FvJGvGtcxti/H+thh3jIijyoyxaZxEA4iIx4F/Aj4r6U8k7ShpG0nz\nge0Lq+4IPBoRv5J0IPBnCW/7OeAMSfsDSNpJ0p8mlNfNjmTnew8D0yV9GPidwvLL8zhmSJoDnFpY\ndjPwlKTTJb1I0jRJr5D0miHE2RhOogFFxLnAP5B9a2/Kp38DTic7PwJ4H/ARSU+RnWxfnvB+VwHn\nAJdJehJYDbxp4B3o7lvAN8k6Hh4g6+woNtk+Aqwn63n7DvAVslqZyH6XejMwP1/+M+ALZM3BsaX8\n5M9sIJL+GlgcEa+rO5a6uCayvkiaLemQvPn6MrIu8KvqjqtO0ydfxWwr25I1W/ci69K/DPhsrRHV\nbGjNOUlHkv3yPQ34QkScPZQ3MqvZUJIov3r4J8AbyU5CbwGOj4i7Sn8zs5oNqzl3ILA2Iu4DkHQZ\n2W8qHZNIkns3rIl+FhG7TbbSsDoW5rB1t+h62n5Zl7RE0kpJK4cUg1mqB3pZqbaOhYhYAawA10Q2\n2oZVE21g68tBds/nmY2dYSXRLcC+kvaStC2wmOwaMrOxM5TmXERskXQq2SUk04ALI+LOYbyXWd0a\ncdmPz4msoW6NiAMmW8mX/ZglchKZJXISmSVq5AWoCz66oO9tbvvQbUOIJE2/+zGMfbh4+cF9b/Ou\nZTeWHkeqfvejyn0Y246F1AO4ffsmJmlVUg/gEU7knjoWGplEkx3AnRKkiQd5ExKx/QDu5QBvyAG8\nlX73o6R9GN0kKkO/ieiaqLvUA7iXA3xISZDKXdxmVWhkTTSuzbnJVNGx4OZcX9ycK0ptzvWiiYlc\nBjfnJtbIJBqXmmgyVZx3uSZKMrpJVIYqOhamSueDa6KJNTKJxqUp1cRzol405ADeSk0/to5uEpWh\n7B9bByljXPjH1omNbRKZlcC/E5lVwUlklmjgJJI0N3+A1V2S7pT0d/n8syRtkHR7Po31s2nMUm6F\n2AKcFhG3SdoRuFXSdfmyT0bEx9PDM2u+gZMoIjaSPTWNiHhK0hp6f/Sh2dgo5ZxI0jzgVcBN+axT\nJa2SdKGkGV228QioNhaSu7gl7UD2SPZ/iYgrJc0ie0JaAP8MzI6I90xShru4rYmG38Ut6QVkj4m/\nJCKuBIiITRHxbEQ8B3yebHB7s7GV0jsn4IvAmoj4RGH+7MJqx5I9W9RsbKX0zh0CvBO4Q9Lt+bxl\nwPH5U7QDWAe8NylCs4bzZT9m3fmyH7MqOInMEjmJzBI5icwSOYnMEjmJzBI5icwSOYnMEjmJzBI5\nicwSOYlKsnz53Fq3b0oZTYihchFR+0R2serIT8uXz+1pXj/bl1FGP9sPaz+qjqGkaWVPx2/dCTRO\nSdT+QbdepxyAqWUMsn3Z+1FXDCVMPSVRI5/ZOsqWLXvwt82RZcseTNq+jDIG2b6MMpoQQ1V8K0SJ\nOrXl+/nwu50LpJbR7wE4jP2oOoaS+FaIOrQ+6Nbffk+SiwdKahmDbl9GGU2IoSrJzTlJ64CngGeB\nLRFxgKSZwJeBeWR3tx4XEY+lvtcoaH3Qg37gxe1Sy0g56MrajzpjqEpZNdEfRcT8QtW3FLg+IvYF\nrs//NxtLw+pYWAQclr++CPgucPqQ3qtR2tvtKc25sspIaUqNcgxVKaMmCuDbkm6VtCSfNysfIRXg\nIWBWCe/TaJ0+4H4+9G7rppbR74E3jP2oOobKlfAbz5z874uBHwELgcfb1nmsw3ZLgJX5VGXfvydP\nvU49/U6UXBNFxIb872bgKrLBGje1xp/L/27usN2KiDigly5EsyZLHQF1+/yJEEjaHjicbLDGa4AT\n8tVOAL6a8j5mTZbasTALuCobDJXpwJci4puSbgEul3QS8ABwXOL7mDWWr1gw685XLJhVwUlklshJ\nZJbIt0KUrP1HwTquXk6NoYwymhBDVVwTlchXLTQnhio5iUrW6VaGQbcvo4yUG+JGPYaquIu7JJN9\nS052EPTyLZtaRi8H4rD3o4oYSuQu7qoVbyJrv6FskO3LKKPf7csoowkxVMkdCyXzGAvNiaEqrolK\n0n7wtyxfPrenA6Db9mWU0ev2ZZTRhBiq5iQyS+QkMkvkJDJL5I6FIUhtt5fR7m9CGU2IoQr+ncis\nO/9OZFYFJ5FZooHPiSS9jGyU05a9gQ8DOwN/BTycz18WEd8YOEKzhivlnEjSNGADcBBwIvB0RHy8\nj+19TmRNVOk50euBeyPigZLKMxsZZSXRYuDSwv+nSlol6UJJM0p6D7NGSk4iSdsCbwX+K591AbAP\nMB/YCJzXZbslklZKWpkag1mdks+JJC0CTomIwzssmwd8LSJeMUkZPieyJurpnKiMKxaOp9CUkzS7\nMJj9sWQjolofUm+FKJaR8ot/ahlNiKEKycMIA28ErizMPlfSHZJWAX8E/H3Ke0wlHmOh3DKq4st+\nGqJ1r0y30X56uZemWxm9bl9GGU2IoUS+7GcUeaCScsuogpPILJFvhWiY9ubKIM2X4jaDNn9Sy2hC\nDFVxTdQQkx0gvY5vkLJ9GWU0IYbKpT5usoyJ+h8r2Ihp+fK5fc3vdd1+ti+jjCbEUNLU0+Mma08g\nJ9HEB8ogB05xm0EPvNQymhBDCVNPSeQubrPu3MVtVgUnkVkiJ5FZIieRWSInkVkiJ5FZIieRWSIn\nkVkiX4DaMH56eLllVME1UYP4ztZyy6hKT0mUD321WdLqwryZkq6TdE/+d0Y+X5I+JWltPmzWgmEF\nP046jSXQfktEv2X0u30ZZTQhhqr1dO2cpIXA08DFrZF7JJ0LPBoRZ0taCsyIiNMlHQX8DXAU2Yio\n50fEQZOU72vnmPgA6bUp062MfppCqWU0IYaSlHftXER8H3i0bfYi4KL89UXAMYX5F0fmRmBnSbN7\ni3lq63aADPLk7kG3L6OMJsRQpZSOhVmFobEeAmblr+cAxb1dn8/bWJiHpCXAkoT3H1tlNF+Kd4Sm\nxjHKMVSij3t+5gGrC/8/3rb8sfzv14DXFuZfDxzg+4n6u3+ml/m9ruub8gaeerqfKKUm2tQaqDFv\nrm3O528Ail8bu+fzrEcevLHcMoYuoSb6GLA0f70UODd/fTRwLSDgYOBm39k6+DfwqN5V2oQYSpjK\nuz2cbJjgjcBvyM5xTgJ2IWuq3QN8B5iZryvgM8C9wB1M0pRzEnlq8OTbw80S+fZwsyo4icwSOYnM\nEjmJzBI5icwSOYnMEjmJzBL5ztYx1IS7SpsQQ1VcE42ZJtxV2oQYquQkGiPFizXbv7UHuat0kDKa\nEEPVnERmiXxONIbK+LZOLaMJMVTFNdEY6dT86TTYxzDLaEIMVXMSmSVyc25MlfGNnVpGE2Kogmsi\ns0ROIrNEkzbnJF0IvBnYXBi48WPAW4Bfk90GfmJEPC5pHrAGuDvf/MaIOHkIcVsH7b1Zg/RupZbR\nhBgq18P4BwuBBWw9SMnhwPT89TnAOZ0GM+ljEJS676X35KnT1NMYC5M256LD6KcR8e2I2JL/eyPZ\nsFhmU1IZ50TvIRsiq2UvST+U9D1Jh3bbSNISSSslrSwhBrPaJHVxSzoT2AJcks/aCOwREY9IejVw\ntaT9I+LJ9m0jYgWwIi8nUuIwq9PANZGkd5N1OPx5tE5sIp6JiEfy17eSdTq8tIQ4zRproCSSdCTw\nQeCtEfGLwvzdJE3LX+8N7AvcV0agZk3VSxf3pcBhwK6S1gP/CJwBbAdcJwme78peCHxE0m+A54CT\nI6L9kSxmY8UjoJp15xFQp7Lly+eWcjtDShlNiKEKronGTBMe9diEGErimmiqmejhyYM+dLjfMpoQ\nQ9WcRGOoeKANetClltGEGCrT73Vuw5io/xqpsZma8KjHJsRQ0lTOtXNmNjEn0ZhZtuzB5KZPahlN\niKFSdTfl3JwbzlT2s18HKaMJMSRObs6ZVcFJNKaa0KRqQgxV8I+tZt35x1azKjiJzBI5icwSOYnM\nEjmJzBI5icwSTZpEki6UtFnS6sK8syRtkHR7Ph1VWHaGpLWS7pZ0xLACN2uKXmqi/wCO7DD/kxEx\nP5++ASBpP2AxsH++zWdbA5eYjauBRkCdwCLgsnzorPuBtcCBCfGZNV7KOdGpklblzb0Z+bw5QPH+\n3fX5vP/HI6DauBg0iS4A9gHmk416el6/BUTEiog4oJfLKsyabKAkiohNEfFsRDwHfJ7nm2wbgOLV\ngrvn88zG1qAjoM4u/Hss0Oq5uwZYLGk7SXuRjYB6c1qIZs026Aioh0maT3bj0jrgvQARcaeky4G7\nyAa6PyUinh1O6GbN4FshzLrzrRBmVXASJfj6219edwh8/e0v32qqK4ZiLFONk8gskZMowdFX/LjW\n9+/0rV91TdCEGOrmJDJL5CRKUPzGnWrfvu2m8v47icwSOYlKUOydsqnHSWSWyEmUyLWPOYnMEjmJ\nRli336mqqh0nep+pVEM7icwSOYkStGqCo6/4cS1XL0ylb/smcxKZJXISmSVyEpWg1ayq+4JUq8eg\nI6B+uTD66TpJt+fz50n6ZWHZ54YZvFkTTDrGAtkIqJ8GLm7NiIh3tF5LOg94orD+vRExv6wAm67O\n2qfTe9fR2dAex1Tr8Jg0iSLi+5LmdVomScBxwB+XG5bZCOnlEePAPGB1h/kLKTymPF/v58APge8B\nh05Q5hJgZT7V/sh6T546TCt7yY9emnMTOR64tPD/RmCPiHhE0quBqyXtHxFPtm8YESuAFeDRfmy0\nDdw7J2k68Dbgy615+UD2j+SvbwXuBV6aGqRZk6V0cb8B+HFErG/NkLRb61EqkvYmGwH1vrQQzZqt\nly7uS4EfAC+TtF7SSfmixWzdlIPsHGlV3uX9FeDkiOj1sSxmI8kjoJp15xFQzargJDJL5CQyS+Qk\nMkvkJJrAgo8uqDsEGwFOoi5aCeREssk4icwSOYk6aK99XBvZRJxEHdz2odsm/N+syElklsiX/bSZ\nrOnmWmlK6emyHydRB50SyckzJTmJBuGayAp8AapZFUaqJjrm+BcPOxSz37r60s091USpYyxUoqrk\n+en+uwOwx53rJ1nTyvK2V+4NwJV3jO4N0G7OmSWatCaSNJds4MZZZMMIrYiI8yXNJBukZB6wDjgu\nIh7Lx6I7HzgK+AXw7oiY8Gx855nTOeyImSn7YVabXmqiLcBpEbEfcDBwiqT9gKXA9RGxL3B9/j/A\nm8gGKNmXbGy5C0qP2qxBJk2iiNjYqkki4ilgDTAHWARclK92EXBM/noRcHFkbgR2ljS79MjNGqKv\nc6J8OOFXATcBsyJiY77oIbLmHmQJ9mBhs/X5vPaylkhaKWnlM796rs+wzZqj5945STsAVwDvj4gn\ns1OfTEREvz+YFkdAnbHLC+rvZ8e9cnUY5V65lp5qIkkvIEugSyLiynz2plYzLf+7OZ+/AZhb2Hz3\nfJ7ZWOpl8EYBXwTWRMQnCouuAU7IX58AfLUw/13KHAw8UWj2mY2dXppzhwDvBO5oPcwLWAacDVye\nj4j6ANkjVgC+Qda9vZasi/vEUiM2a5henk/0v4C6LH59h/UDOCUxLrOR4SsWzBI5icwSOYnMEjmJ\nzBI15X6ih8me9fqzumMp0a6Mz/6M075A7/uzZ0TsNtlKjUgiAEkre7kBalSM0/6M075A+fvj5pxZ\nIieRWaImJdGKugMo2TjtzzjtC5S8P405JzIbVU2qicxGkpPILFHtSSTpSEl3S1oraenkWzSPpHWS\n7pB0u6SV+byZkq6TdE/+d0bdcXYj6UJJmyWtLszrGH9+i8un8s9rlaTGPXemy/6cJWlD/hndLumo\nwrIz8v25W9IRfb9hRNQ2AdOAe4G9gW2BHwH71RnTgPuxDti1bd65wNL89VLgnLrjnCD+hcACYPVk\n8ZPd5nIt2ZX9BwM31R1/j/tzFvCBDuvulx932wF75cfjtH7er+6a6EBgbUTcFxG/Bi4jG+hkHHQb\nyKVxIuL7wKNts0d2IJou+9PNIuCyiHgmIu4nuw/uwH7er+4k6mlQkxEQwLcl3SppST6v20AuoyJp\nIJqGOjVvgl5YaF4n70/dSTQuXhsRC8jG3DtF0sLiwsjaDSP7W8Kox5+7ANgHmA9sBM4rq+C6k2gs\nBjWJiA35383AVWTNgW4DuYyKsRqIJiI2RcSzEfEc8Hmeb7Il70/dSXQLsK+kvSRtCywmG+hkZEja\nXtKOrdfA4cBqug/kMirGaiCatvO2Y8k+I8j2Z7Gk7STtRTZy7819Fd6AnpSjgJ+Q9YqcWXc8A8S/\nN1nvzo+AO1v7AOxCNrzyPcB3gJl1xzrBPlxK1sT5Ddk5wUnd4ifrlftM/nndARxQd/w97s9/5vGu\nyhNndmH9M/P9uRt4U7/v58t+zBLV3ZwzG3lOIrNETiKzRE4is0ROIrNETiKzRE4is0T/Bw0rcdmC\nuFkuAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "(42, 42, 4)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAACDCAYAAACdg+BGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqwgXdDOzivh/H0M7eyQWeTsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "4MMO9W-H5LbX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Agent():\n",
        "    def __init__(self, session, action_size, model='mnih',\n",
        "        optimizer=tf.train.AdamOptimizer(1e-4)):\n",
        "\n",
        "        self.action_size = action_size\n",
        "        self.optimizer = optimizer\n",
        "        self.sess = session\n",
        "\n",
        "        with tf.variable_scope('network'):\n",
        "            self.action = tf.placeholder('int32', [None], name='action')\n",
        "            self.target_value = tf.placeholder('float32', [None], name='target_value')\n",
        "            if model == 'mnih':\n",
        "                self.state, self.policy, self.value = self.build_model(42, 42, 4)\n",
        "            else:\n",
        "                # Assume we wanted a feedforward neural network\n",
        "                self.state, self.policy, self.value = self.build_model_feedforward(3)\n",
        "            self.weights = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
        "            scope='network')\n",
        "            self.advantages = tf.placeholder('float32', [None], name='advantages')\n",
        "\n",
        "        with tf.variable_scope('optimizer'):\n",
        "            # Compute the one hot vectors for each action given.\n",
        "            action_one_hot = tf.one_hot(self.action, self.action_size, 1.0, 0.0)\n",
        "\n",
        "            min_policy = 1e-8\n",
        "            max_policy = 1.0 - 1e-8\n",
        "            self.log_policy = tf.log(tf.clip_by_value(self.policy, 0.000001, 0.999999))\n",
        "\n",
        "            # For a given state and action, compute the log of the policy at\n",
        "            # that action for that state. This also works on batches.\n",
        "            self.log_pi_for_action = tf.reduce_sum(tf.multiply(self.log_policy, action_one_hot), reduction_indices=1)\n",
        "\n",
        "            # Takes in R_t - V(s_t) as in the async paper. Note that we feed in\n",
        "            # the advantages so that V(s_t) is treated as a constant for the\n",
        "            # gradient. This is because V(s_t) is the baseline (called 'b' in\n",
        "            # the REINFORCE algorithm). As long as the baseline is constant wrt\n",
        "            # the parameters we are optimising (in this case those for the\n",
        "            # policy), then the expected value of grad_theta log pi * b is zero,\n",
        "            # so the choice of b doesn't affect the expectation. It reduces the\n",
        "            # variance though.\n",
        "            # We want to do gradient ascent on the expected discounted reward.\n",
        "            # The gradient of the expected discounted reward is the gradient of\n",
        "            # log pi * (R - estimated V), where R is the sampled reward from the\n",
        "            # given state following the policy pi. Since we want to maximise\n",
        "            # this, we define the policy loss as the negative and get tensorflow\n",
        "            # to do the automatic differentiation for us.\n",
        "            self.policy_loss = -tf.reduce_mean(self.log_pi_for_action * self.advantages)\n",
        "\n",
        "            # The value loss is much easier to understand: we want our value\n",
        "            # function to accurately estimated the sampled discounted rewards,\n",
        "            # so we just impose a square error loss.\n",
        "            # Note that the target value should be the discounted reward for the\n",
        "            # state as just sampled.\n",
        "            self.value_loss = tf.reduce_mean(tf.square(self.target_value - self.value))\n",
        "\n",
        "            # We follow Mnih's paper and introduce the entropy as another loss\n",
        "            # to the policy. The entropy of a probability distribution is just\n",
        "            # the expected value of - log P(X), denoted E(-log P(X)), which we\n",
        "            # can compute for our policy at any given state with\n",
        "            # sum(policy * -log(policy)), as below. This will be a positive\n",
        "            # number, since self.policy contains numbers between 0 and 1, so the\n",
        "            # log is negative. Note that entropy is smaller when the probability\n",
        "            # distribution is more concentrated on one action, so a larger\n",
        "            # entropy implies more exploration. Thus we penalise small entropy,\n",
        "            # or equivalently, add -entropy to our loss.\n",
        "            self.entropy = tf.reduce_sum(tf.multiply(self.policy, -self.log_policy))\n",
        "\n",
        "            # Try to minimise the loss. There is some rationale for choosing the\n",
        "            # weighted linear combination here that I found somewhere else that\n",
        "            # I can't remember, but I haven't tried to optimise it.\n",
        "            # Note the negative entropy term, which encourages exploration:\n",
        "            # higher entropy corresponds to less certainty.\n",
        "            self.loss = 0.5 * self.value_loss + self.policy_loss - self.entropy\\\n",
        "            * 0.01\n",
        "\n",
        "            # Compute the gradient of the loss with respect to all the weights,\n",
        "            # and create a list of tuples consisting of the gradient to apply to\n",
        "            # the weight.\n",
        "            grads = tf.gradients(self.loss, self.weights)\n",
        "            grads, _ = tf.clip_by_global_norm(grads, 40.0)\n",
        "            grads_vars = list(zip(grads, self.weights))\n",
        "\n",
        "            # Create an operator to apply the gradients using the optimizer.\n",
        "            # Note that apply_gradients is the second part of minimize() for the\n",
        "            # optimizer, so will minimize the loss.\n",
        "            self.train_op = optimizer.apply_gradients(grads_vars)\n",
        "\n",
        "    def get_policy(self, state):\n",
        "        return self.sess.run(self.policy, {self.state: state}).flatten()\n",
        "\n",
        "    def get_value(self, state):\n",
        "        return self.sess.run(self.value, {self.state: state}).flatten()\n",
        "\n",
        "    def get_policy_and_value(self, state):\n",
        "        policy, value = self.sess.run([self.policy, self.value], {self.state:\n",
        "        state})\n",
        "        return policy.flatten(), value.flatten()\n",
        "\n",
        "    # Train the network on the given states and rewards\n",
        "    def train(self, states, actions, target_values, advantages):\n",
        "        # Training\n",
        "        self.sess.run(self.train_op, feed_dict={\n",
        "            self.state: states,\n",
        "            self.action: actions,\n",
        "            self.target_value: target_values,\n",
        "            self.advantages: advantages\n",
        "        })\n",
        "\n",
        "    # Builds the DQN model as in Mnih, but we get a softmax output for the\n",
        "    # policy from fc1 and a linear output for the value from fc1.\n",
        "    def build_model(self, h, w, channels):\n",
        "        self.layers = {}\n",
        "        state = tf.placeholder('float32', shape=(h, w, channels), name='state')\n",
        "        self.layers['state'] = state\n",
        "        # First convolutional layer\n",
        "        with tf.variable_scope('conv1'):\n",
        "            conv1 = tf.contrib.layers.convolution2d(inputs=state,\n",
        "            num_outputs=16, kernel_size=[8,8], stride=[4,4], padding=\"VALID\",\n",
        "            activation_fn=tf.nn.relu,\n",
        "            weights_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
        "            biases_initializer=tf.zeros_initializer())\n",
        "            self.layers['conv1'] = conv1\n",
        "\n",
        "        # Second convolutional layer\n",
        "        with tf.variable_scope('conv2'):\n",
        "            conv2 = tf.contrib.layers.convolution2d(inputs=conv1, num_outputs=32,\n",
        "            kernel_size=[4,4], stride=[2,2], padding=\"VALID\",\n",
        "            activation_fn=tf.nn.relu,\n",
        "            weights_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
        "            biases_initializer=tf.zeros_initializer())\n",
        "            self.layers['conv2'] = conv2\n",
        "\n",
        "        # Flatten the network\n",
        "        with tf.variable_scope('flatten'):\n",
        "            flatten = tf.contrib.layers.flatten(inputs=conv2)\n",
        "            self.layers['flatten'] = flatten\n",
        "\n",
        "        # Fully connected layer with 256 hidden units\n",
        "        with tf.variable_scope('fc1'):\n",
        "            fc1 = tf.contrib.layers.fully_connected(inputs=flatten, num_outputs=256,\n",
        "            activation_fn=tf.nn.relu,\n",
        "            weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
        "            biases_initializer=tf.zeros_initializer())\n",
        "            self.layers['fc1'] = fc1\n",
        "\n",
        "        # The policy output\n",
        "        with tf.variable_scope('policy'):\n",
        "            policy = tf.contrib.layers.fully_connected(inputs=fc1,\n",
        "            num_outputs=self.action_size, activation_fn=tf.nn.softmax,\n",
        "            weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
        "            biases_initializer=None)\n",
        "            self.layers['policy'] = policy\n",
        "\n",
        "        # The value output\n",
        "        with tf.variable_scope('value'):\n",
        "            value = tf.contrib.layers.fully_connected(inputs=fc1, num_outputs=1,\n",
        "            activation_fn=None,\n",
        "            weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
        "            biases_initializer=None)\n",
        "            self.layers['value'] = value\n",
        "\n",
        "        return state, policy, value\n",
        "\n",
        "    # Builds a simple feedforward model to learn the cart and pole environment.\n",
        "    def build_model_feedforward(self, input_dim, num_hidden=30):\n",
        "        self.layers = {}\n",
        "        state = tf.placeholder('float32', shape=(None, input_dim), name='state')\n",
        "\n",
        "        self.layers['state'] = state\n",
        "        # Fully connected layer with num_hidden hidden units\n",
        "        with tf.variable_scope('fc1'):\n",
        "            fc1 = tf.contrib.layers.fully_connected(inputs=state,\n",
        "            num_outputs=num_hidden,\n",
        "            activation_fn=tf.nn.relu,\n",
        "            weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
        "            biases_initializer=tf.zeros_initializer())\n",
        "            self.layers['fc1'] = fc1\n",
        "\n",
        "        # Fully connected layer with num_hidden hidden units\n",
        "        with tf.variable_scope('fc2'):\n",
        "            fc2 = tf.contrib.layers.fully_connected(inputs=fc1,\n",
        "            num_outputs=num_hidden,\n",
        "            activation_fn=tf.nn.relu,\n",
        "            weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
        "            biases_initializer=tf.zeros_initializer())\n",
        "            self.layers['fc2'] = fc2\n",
        "\n",
        "        # The policy output to the two possible actions\n",
        "        with tf.variable_scope('policy'):\n",
        "            policy = tf.contrib.layers.fully_connected(inputs=fc2,\n",
        "            num_outputs=self.action_size, activation_fn=tf.nn.softmax,\n",
        "            weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
        "            biases_initializer=tf.zeros_initializer())\n",
        "            self.layers['policy'] = policy\n",
        "\n",
        "        # The value output\n",
        "        with tf.variable_scope('value'):\n",
        "            value = tf.contrib.layers.fully_connected(inputs=fc2, num_outputs=1,\n",
        "            activation_fn=None,\n",
        "            weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
        "            biases_initializer=tf.zeros_initializer())\n",
        "            self.layers['value'] = value\n",
        "\n",
        "        return state, policy, value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y5790o-ORfn2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Summary:\n",
        "    def __init__(self, logdir, agent):\n",
        "        with tf.variable_scope('summary'):\n",
        "            summarising = ['episode_avg_reward', 'avg_value']\n",
        "            self.agent = agent\n",
        "            self.writer = tf.summary.FileWriter(logdir, self.agent.sess.graph)\n",
        "            self.summary_ops = {}\n",
        "            self.summary_vars = {}\n",
        "            self.summary_ph = {}\n",
        "            for s in summarising:\n",
        "                self.summary_vars[s] = tf.Variable(0.0)\n",
        "                self.summary_ph[s] = tf.placeholder('float32', name=s)\n",
        "                self.summary_ops[s] = tf.summary.scalar(s, self.summary_vars[s])\n",
        "            self.update_ops = []\n",
        "            for k in self.summary_vars:\n",
        "                self.update_ops.append(self.summary_vars[k].assign(self.summary_ph[k]))\n",
        "            self.summary_op = tf.summary.merge(list(self.summary_ops.values()))\n",
        "\n",
        "    def write_summary(self, summary, t):\n",
        "        self.agent.sess.run(self.update_ops, {self.summary_ph[k]: v for k, v in summary.items()})\n",
        "        summary_to_add = self.agent.sess.run(self.summary_op, {self.summary_vars[k]: v for k, v in summary.items()})\n",
        "        self.writer.add_summary(summary_to_add, global_step=t)\n",
        "\n",
        "def async_trainer(agent, env, sess, thread_idx, T_queue, summary, saver,\n",
        "    save_path):\n",
        "    print (\"Training thread\", thread_idx)\n",
        "    T = T_queue.get()\n",
        "    T_queue.put(T+1)\n",
        "    t = 0\n",
        "\n",
        "    last_verbose = T\n",
        "    last_time = time()\n",
        "    last_target_update = T\n",
        "\n",
        "    terminal = True\n",
        "    while T < T_MAX:\n",
        "        t_start = t\n",
        "        batch_states = []\n",
        "        batch_rewards = []\n",
        "        batch_actions = []\n",
        "        baseline_values = []\n",
        "\n",
        "        if terminal:\n",
        "            terminal = False\n",
        "            state = env.reset()\n",
        "\n",
        "        while not terminal and len(batch_states) < I_ASYNC_UPDATE:\n",
        "            # Save the current state\n",
        "            batch_states.append(state)\n",
        "\n",
        "            # Choose an action randomly according to the policy\n",
        "            # probabilities. We do this anyway to prevent us having to compute\n",
        "            # the baseline value separately.\n",
        "            policy, value = agent.get_policy_and_value(state)\n",
        "            action_idx = np.random.choice(agent.action_size, p=policy)\n",
        "\n",
        "            # Take the action and get the next state, reward and terminal.\n",
        "            state, reward, terminal, _ = env.step(action_idx)\n",
        "\n",
        "            # Update counters\n",
        "            t += 1\n",
        "            T = T_queue.get()\n",
        "            T_queue.put(T+1)\n",
        "\n",
        "            # Clip the reward to be between -1 and 1\n",
        "            reward = np.clip(reward, -1, 1)\n",
        "\n",
        "            # Save the rewards and actions\n",
        "            batch_rewards.append(reward)\n",
        "            batch_actions.append(action_idx)\n",
        "            baseline_values.append(value[0])\n",
        "\n",
        "        target_value = 0\n",
        "        # If the last state was terminal, just put R = 0. Else we want the\n",
        "        # estimated value of the last state.\n",
        "        if not terminal:\n",
        "            target_value = agent.get_value(state)[0]\n",
        "        last_R = target_value\n",
        "\n",
        "        # Compute the sampled n-step discounted reward\n",
        "        batch_target_values = []\n",
        "        for reward in reversed(batch_rewards):\n",
        "            target_value = reward + DISCOUNT_FACTOR * target_value\n",
        "            batch_target_values.append(target_value)\n",
        "        # Reverse the batch target values, so they are in the correct order\n",
        "        # again.\n",
        "        batch_target_values.reverse()\n",
        "\n",
        "        # Test batch targets\n",
        "        if TESTING:\n",
        "            temp_rewards = batch_rewards + [last_R]\n",
        "            test_batch_target_values = []\n",
        "            for j in range(len(batch_rewards)):\n",
        "                test_batch_target_values.append(discount(temp_rewards[j:], DISCOUNT_FACTOR))\n",
        "            if not test_equals(batch_target_values, test_batch_target_values,\n",
        "                1e-5):\n",
        "                print (\"Assertion failed\")\n",
        "                print (last_R)\n",
        "                print (batch_rewards)\n",
        "                print (batch_target_values)\n",
        "                print (test_batch_target_values)\n",
        "\n",
        "        # Compute the estimated value of each state\n",
        "        batch_advantages = np.array(batch_target_values) - np.array(baseline_values)\n",
        "\n",
        "        # Apply asynchronous gradient update\n",
        "        agent.train(np.vstack(batch_states), batch_actions, batch_target_values,\n",
        "        batch_advantages)\n",
        "\n",
        "    global training_finished\n",
        "    training_finished = True\n",
        "\n",
        "def estimate_reward(agent, env, episodes=10, max_steps=10000):\n",
        "    episode_rewards = []\n",
        "    episode_vals = []\n",
        "    t = 0\n",
        "    for i in range(episodes):\n",
        "        episode_reward = 0\n",
        "        state = env.reset()\n",
        "        terminal = False\n",
        "        while not terminal:\n",
        "            policy, value = agent.get_policy_and_value(state)\n",
        "            action_idx = np.random.choice(agent.action_size, p=policy)\n",
        "            state, reward, terminal, _ = env.step(action_idx)\n",
        "            t += 1\n",
        "            episode_vals.append(value)\n",
        "            episode_reward += reward\n",
        "            if t > max_steps:\n",
        "                episode_rewards.append(episode_reward)\n",
        "                return episode_rewards, episode_vals\n",
        "        episode_rewards.append(episode_reward)\n",
        "    return episode_rewards, episode_vals\n",
        "\n",
        "def evaluator(agent, env, sess, T_queue, summary, saver, save_path):\n",
        "    # Read T and put the same T back on.\n",
        "    T = T_queue.get()\n",
        "    T_queue.put(T)\n",
        "    last_time = time()\n",
        "    last_verbose = T\n",
        "    while T < T_MAX:\n",
        "        T = T_queue.get()\n",
        "        T_queue.put(T)\n",
        "        if T - last_verbose >= VERBOSE_EVERY:\n",
        "            print (\"T\", T)\n",
        "            current_time = time()\n",
        "            print (\"Train steps per second\", float(T - last_verbose) / (current_time - last_time))\n",
        "            last_time = current_time\n",
        "            last_verbose = T\n",
        "\n",
        "            print (\"Evaluating agent\")\n",
        "            episode_rewards, episode_vals = estimate_reward(agent, env, episodes=5)\n",
        "            avg_ep_r = np.mean(episode_rewards)\n",
        "            avg_val = np.mean(episode_vals)\n",
        "            print (\"Avg ep reward\", avg_ep_r, \"Average value\", avg_val)\n",
        "\n",
        "            summary.write_summary({'episode_avg_reward': avg_ep_r, 'avg_value': avg_val}, T)\n",
        "            checkpoint_file = saver.save(sess, save_path, global_step=T)\n",
        "            print (\"Saved in\", checkpoint_file)\n",
        "        sleep(1.0)\n",
        "\n",
        "# If restore is True, then start the model from the most recent checkpoint.\n",
        "# Else initialise as usual.\n",
        "def a3c(game_name, num_threads=8, restore=None, save_path='model'):\n",
        "    processes = []\n",
        "    envs = []\n",
        "    for _ in range(num_threads+1):\n",
        "        gym_env = gym.make(game_name)\n",
        "        # if game_name == 'CartPole-v0':\n",
        "        #     env = CustomGymClassicControl(game_name)\n",
        "        # else:\n",
        "        print (\"Assuming ATARI game and playing with pixels\")\n",
        "        env = make_env(game_name)\n",
        "        envs.append(env)\n",
        "\n",
        "    # Separate out the evaluation environment\n",
        "    evaluation_env = envs[0]\n",
        "    envs = envs[1:]\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        agent = Agent(session=sess,\n",
        "        action_size=envs[0].action_size, model='mnih',\n",
        "        optimizer=tf.train.AdamOptimizer(INITIAL_LEARNING_RATE))\n",
        "\n",
        "        # Create a saver, and only keep 2 checkpoints.\n",
        "        saver = tf.train.Saver(max_to_keep=2)\n",
        "\n",
        "        T_queue = queue.Queue()\n",
        "\n",
        "        # Either restore the parameters or don't.\n",
        "        if restore is not None:\n",
        "            saver.restore(sess, save_path + '-' + str(restore))\n",
        "            last_T = restore\n",
        "            print (\"T was:\", last_T)\n",
        "            T_queue.put(last_T)\n",
        "        else:\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "            T_queue.put(0)\n",
        "\n",
        "        summary = Summary(save_path, agent)\n",
        "\n",
        "        # Create a process for each worker\n",
        "        for i in range(num_threads):\n",
        "            processes.append(threading.Thread(target=async_trainer, args=(agent,\n",
        "            envs[i], sess, i, T_queue, summary, saver, save_path,)))\n",
        "\n",
        "        # Create a process to evaluate the agent\n",
        "        processes.append(threading.Thread(target=evaluator, args=(agent,\n",
        "        evaluation_env, sess, T_queue, summary, saver, save_path,)))\n",
        "\n",
        "        # Start all the processes\n",
        "        for p in processes:\n",
        "            p.daemon = True\n",
        "            p.start()\n",
        "\n",
        "        # Until training is finished\n",
        "        while not training_finished:\n",
        "            sleep(0.01)\n",
        "\n",
        "        # Join the processes, so we get this thread back.\n",
        "        for p in processes:\n",
        "            p.join()\n",
        "\n",
        "# Returns sum(rewards[i] * gamma**i)\n",
        "def discount(rewards, gamma):\n",
        "    return np.sum([rewards[i] * gamma**i for i in range(len(rewards))])\n",
        "\n",
        "def test_equals(arr1, arr2, eps):\n",
        "    return np.sum(np.abs(np.array(arr1)-np.array(arr2))) < eps\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L4BRzUAA5u5u",
        "colab_type": "code",
        "outputId": "938999b4-6152-4006-be03-70fe63b0fa82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        }
      },
      "cell_type": "code",
      "source": [
        "a3c(game_name='SpaceInvaders-v0')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Assuming ATARI game and playing with pixels\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Assuming ATARI game and playing with pixels\n",
            "Assuming ATARI game and playing with pixels\n",
            "Assuming ATARI game and playing with pixels\n",
            "Assuming ATARI game and playing with pixels\n",
            "Assuming ATARI game and playing with pixels\n",
            "Assuming ATARI game and playing with pixels\n",
            "Assuming ATARI game and playing with pixels\n",
            "Assuming ATARI game and playing with pixels\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-8e6b14b707b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma3c\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgame_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'SpaceInvaders-v0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-1597e4c17491>\u001b[0m in \u001b[0;36ma3c\u001b[0;34m(game_name, num_threads, restore, save_path)\u001b[0m\n\u001b[1;32m    180\u001b[0m         agent = Agent(session=sess,\n\u001b[1;32m    181\u001b[0m         \u001b[0maction_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mnih'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         optimizer=tf.train.AdamOptimizer(INITIAL_LEARNING_RATE))\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;31m# Create a saver, and only keep 2 checkpoints.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-be59af643cc7>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, session, action_size, model, optimizer)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'target_value'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'mnih'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0;31m# Assume we wanted a feedforward neural network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-be59af643cc7>\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(self, h, w, channels)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mactivation_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mweights_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxavier_initializer_conv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             biases_initializer=tf.zeros_initializer())\n\u001b[0m\u001b[1;32m    121\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'conv1'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\u001b[0m in \u001b[0;36mfunc_with_args\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m       \u001b[0mcurrent_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_scope\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey_func\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m       \u001b[0mcurrent_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcurrent_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m   \u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/layers/python/layers/layers.py\u001b[0m in \u001b[0;36mconvolution2d\u001b[0;34m(inputs, num_outputs, kernel_size, stride, padding, data_format, rate, activation_fn, normalizer_fn, normalizer_params, weights_initializer, weights_regularizer, biases_initializer, biases_regularizer, reuse, variables_collections, outputs_collections, trainable, scope)\u001b[0m\n\u001b[1;32m   1153\u001b[0m                      \u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m                      \u001b[0mscope\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1155\u001b[0;31m                      conv_dims=2)\n\u001b[0m\u001b[1;32m   1156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1157\u001b[0m \u001b[0mconvolution2d\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvolution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\u001b[0m in \u001b[0;36mfunc_with_args\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m       \u001b[0mcurrent_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_scope\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey_func\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m       \u001b[0mcurrent_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcurrent_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m   \u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/layers/python/layers/layers.py\u001b[0m in \u001b[0;36mconvolution\u001b[0;34m(inputs, num_outputs, kernel_size, stride, padding, data_format, rate, activation_fn, normalizer_fn, normalizer_params, weights_initializer, weights_regularizer, biases_initializer, biases_regularizer, reuse, variables_collections, outputs_collections, trainable, scope, conv_dims)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconv_dims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconv_dims\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minput_rank\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m       raise ValueError('Convolution expects input with rank %d, got %d' %\n\u001b[0;32m-> 1026\u001b[0;31m                        (conv_dims + 2, input_rank))\n\u001b[0m\u001b[1;32m   1027\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput_rank\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m       \u001b[0mlayer_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvolutional_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConvolution1D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Convolution expects input with rank 4, got 3"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "IgULMtwb6ZlW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}