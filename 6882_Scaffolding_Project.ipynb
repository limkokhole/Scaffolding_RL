{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "6882_Scaffolding_Project.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/serenabooth/Scaffolding_RL/blob/master/6882_Scaffolding_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9-o5V5dSftS",
        "colab_type": "text"
      },
      "source": [
        "Code is modified from https://cgnicholls.github.io/reinforcement-learning/2017/03/27/a3c.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfDH4nrQSlnj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1445
        },
        "outputId": "1c3c5c4c-a34a-476c-e36e-210113b975a9"
      },
      "source": [
        "# install all the packages\n",
        "!apt-get update\n",
        "!apt-get install -y python-numpy python-dev cmake zlib1g-dev libjpeg-dev xvfb ffmpeg xorg-dev python-opengl libboost-all-dev libsdl2-dev swig\n",
        "!apt-get install python-opengl -y\n",
        "!apt install xvfb -y\n",
        "!pip install gym\n",
        "!pip install \"gym[atari]\"\n",
        "!pip install tqdm\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install piglet"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rGet:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\r0% [Waiting for headers] [1 InRelease 2,586 B/88.7 kB 3%] [Connecting to cloud.\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "\r0% [1 InRelease 11.3 kB/88.7 kB 13%] [Connecting to cloud.r-project.org (52.85.\r                                                                               \rHit:3 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "\r0% [Waiting for headers] [1 InRelease 11.3 kB/88.7 kB 13%] [Connected to cloud.\r                                                                               \rGet:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "\r0% [4 InRelease 2,601 B/88.7 kB 3%] [1 InRelease 11.3 kB/88.7 kB 13%] [Connecte\r0% [2 InRelease gpgv 242 kB] [4 InRelease 2,601 B/88.7 kB 3%] [1 InRelease 11.3\r                                                                               \rHit:5 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease\n",
            "\r0% [2 InRelease gpgv 242 kB] [4 InRelease 22.9 kB/88.7 kB 26%] [1 InRelease 43.\r0% [2 InRelease gpgv 242 kB] [4 InRelease 48.9 kB/88.7 kB 55%] [Connected to cl\r0% [2 InRelease gpgv 242 kB] [Waiting for headers] [Waiting for headers] [Waiti\r                                                                               \rGet:6 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:7 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease\n",
            "Ign:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:11 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Fetched 252 kB in 2s (151 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "cmake is already the newest version (3.10.2-1ubuntu2).\n",
            "libjpeg-dev is already the newest version (8c-2ubuntu8).\n",
            "python-dev is already the newest version (2.7.15~rc1-1).\n",
            "python-numpy is already the newest version (1:1.13.3-2ubuntu1).\n",
            "zlib1g-dev is already the newest version (1:1.2.11.dfsg-0ubuntu2).\n",
            "libboost-all-dev is already the newest version (1.65.1.0ubuntu1).\n",
            "python-opengl is already the newest version (3.1.0+dfsg-1).\n",
            "swig is already the newest version (3.0.12-1).\n",
            "xorg-dev is already the newest version (1:7.7+19ubuntu7.1).\n",
            "ffmpeg is already the newest version (7:3.4.4-0ubuntu0.18.04.1).\n",
            "libsdl2-dev is already the newest version (2.0.8+dfsg1-1ubuntu1.18.04.3).\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.2).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 58 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python-opengl is already the newest version (3.1.0+dfsg-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 58 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.2).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 58 not upgraded.\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.10.11)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.2.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.16.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym) (1.12.0)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.2)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (2.21.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym) (0.16.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (2019.3.9)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym) (1.24.2)\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.6/dist-packages (0.10.11)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.16.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.12.0)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (2.21.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.2.1)\n",
            "Requirement already satisfied: pyglet>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.3.2)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (4.3.0)\n",
            "Requirement already satisfied: atari-py>=0.1.4; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (0.1.7)\n",
            "Requirement already satisfied: PyOpenGL; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (3.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[atari]) (2019.3.9)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[atari]) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[atari]) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym[atari]) (1.24.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet>=1.2.0->gym[atari]) (0.16.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow; extra == \"atari\"->gym[atari]) (0.46)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.6/dist-packages (0.2.1)\n",
            "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.6/dist-packages (from pyvirtualdisplay) (0.2.5)\n",
            "Requirement already satisfied: piglet in /usr/local/lib/python3.6/dist-packages (0.4.4)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from piglet) (19.1.0)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.6/dist-packages (from piglet) (1.1.1)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.6/dist-packages (from piglet) (1.6.2)\n",
            "Requirement already satisfied: Parsley in /usr/local/lib/python3.6/dist-packages (from piglet) (1.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet) (0.33.1)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKg_2u98SqCc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "98c14e23-4201-4e6d-8e89-db795ab3b340"
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "from __future__ import print_function, division\n",
        "from IPython.core import display\n",
        "from IPython.display import HTML, clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import gym\n",
        "from PIL import Image, ImageDraw\n",
        "from gym.core import Wrapper\n",
        "from gym.spaces.box import Box\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.layers import Conv2D, Dense, Flatten, Input, LSTM, Reshape\n",
        "from keras.models import Model, Sequential\n",
        "from tqdm import trange\n",
        "from google.colab import files\n",
        "import datetime\n",
        "from time import time, sleep, gmtime, strftime\n",
        "import uuid\n",
        "import pickle\n",
        "import random\n",
        "import queue\n",
        "# import multiprocessing\n",
        "import threading"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rh5c3yyTS-K6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# used to enable Tensorboard\n",
        "! wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip > /dev/null 2>&1\n",
        "! unzip ngrok-stable-linux-amd64.zip > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbA-iTnTTLrD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LOG_DIR = './log'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IltkY9gTOYE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b7d37646-5237-4e57-8d48-3cdf6901fd1e"
      },
      "source": [
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://54666631.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qHq-WAFTS3R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eacfce64-9ed0-4ae6-cf7a-daae918573d5"
      },
      "source": [
        "# mount Google drive - to store model checkpoints, etc\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMLTH5Fyb21m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random.seed(100)\n",
        "\n",
        "# FLAGS\n",
        "T_MAX = 100000000\n",
        "NUM_THREADS = 8\n",
        "INITIAL_LEARNING_RATE = 1e-4\n",
        "DISCOUNT_FACTOR = 0.99\n",
        "VERBOSE_EVERY = 40000\n",
        "TESTING = False\n",
        "\n",
        "I_ASYNC_UPDATE = 5\n",
        "\n",
        "FLAGS = {\"T_MAX\": T_MAX, \"NUM_THREADS\": NUM_THREADS, \"INITIAL_LEARNING_RATE\":\n",
        "INITIAL_LEARNING_RATE, \"DISCOUNT_FACTOR\": DISCOUNT_FACTOR, \"VERBOSE_EVERY\":\n",
        "VERBOSE_EVERY, \"TESTING\": TESTING, \"I_ASYNC_UPDATE\": I_ASYNC_UPDATE}\n",
        "\n",
        "training_finished = False\n",
        "last_T = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZW9AKs_lTaQS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "if os.environ.get(\"DISPLAY\") is str and len(os.environ.get(\"DISPLAY\"))!=0:\n",
        "    !bash ../xvfb start\n",
        "    %env DISPLAY=:1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Wup_JPyTjlB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class CustomGym:\n",
        "#     def __init__(self, game_name, skip_actions=4, num_frames=4, w=64, h=64, crop=lambda img: img):\n",
        "#         self.env = gym.make(game_name)\n",
        "#         self.num_frames = num_frames\n",
        "#         self.skip_actions = skip_actions\n",
        "#         self.w = w\n",
        "#         self.h = h\n",
        "#         self.crop = crop\n",
        "#         if game_name == 'SpaceInvaders-v0':\n",
        "#             self.action_space = [1,2,3] # For space invaders\n",
        "#         elif game_name == 'Pong-v0':\n",
        "#             self.action_space = [1,2,3]\n",
        "#         elif game_name == 'Breakout-v0':\n",
        "#             self.action_space = [1,4,5]\n",
        "#         else:\n",
        "#             # Use the actions specified by Open AI. Sometimes this has more\n",
        "#             # actions than we want, and some actions do the same thing.\n",
        "#             self.action_space = range(env.action_space.n)\n",
        "\n",
        "#         self.action_size = len(self.action_space)\n",
        "#         self.observation_shape = self.env.observation_space.shape\n",
        "\n",
        "#         self.state = None\n",
        "#         self.game_name = game_name\n",
        "\n",
        "#     def preprocess(self, obs, is_start=False):\n",
        "#         obs = self.crop(obs)\n",
        "#         grayscale = obs.astype('float32').mean(2)\n",
        "#         # s = imresize(grayscale, (self.w, self.h)).astype('float32') * (1.0/255.0)\n",
        "#         grayscale = Image.fromarray(grayscale).resize((self.w, self.h))\n",
        "#         s = np.array(grayscale).astype('float32') / 255.\n",
        "#         s = s.reshape(1, s.shape[0], s.shape[1], 1)\n",
        "#         if is_start or self.state is None:\n",
        "#             self.state = np.repeat(s, self.num_frames, axis=3)\n",
        "#         else:\n",
        "#             self.state = np.append(s, self.state[:,:,:,:self.num_frames-1], axis=3)\n",
        "#         return self.state\n",
        "\n",
        "#     def render(self):\n",
        "#         self.env.render()\n",
        "\n",
        "#     def reset(self):\n",
        "#         return self.preprocess(self.env.reset(), is_start=True)\n",
        "\n",
        "#     def step(self, action_idx):\n",
        "#         action = self.action_space[action_idx]\n",
        "#         accum_reward = 0\n",
        "#         prev_s = None\n",
        "#         for _ in range(self.skip_actions):\n",
        "#             s, r, term, info = self.env.step(action)\n",
        "#             accum_reward += r\n",
        "#             if term:\n",
        "#                 break\n",
        "#             prev_s = s\n",
        "#         # Takes maximum value for each pixel value over the current and previous\n",
        "#         # frame. Used to get round Atari sprites flickering (Mnih et al. (2015))\n",
        "#         if self.game_name == 'SpaceInvaders-v0' and prev_s is not None:\n",
        "#             s = np.maximum.reduce([s, prev_s])\n",
        "#         return self.preprocess(s), accum_reward, term, info\n",
        "\n",
        "\n",
        "class CustomGym(Wrapper):\n",
        "    def __init__(self, env, game_name, skip_actions=4, num_frames=4, w=64, h=64, crop=lambda img: img):\n",
        "        \"\"\"A gym wrapper that reshapes, crops and scales image into the desired shapes\"\"\"\n",
        "        super(CustomGym, self).__init__(env)\n",
        "        self.num_frames = num_frames\n",
        "        self.skip_actions = skip_actions\n",
        "        self.w = w\n",
        "        self.h = h\n",
        "        self.crop = crop\n",
        "        if game_name == 'SpaceInvaders-v0':\n",
        "            self.action_space = [1,2,3] # For space invaders\n",
        "        elif game_name == 'Pong-v0':\n",
        "            self.action_space = [1,2,3]\n",
        "        elif game_name == 'Breakout-v0':\n",
        "            self.action_space = [1,4,5]\n",
        "        else:\n",
        "            # Use the actions specified by Open AI. Sometimes this has more\n",
        "            # actions than we want, and some actions do the same thing.\n",
        "            self.action_space = range(env.action_space.n)\n",
        "\n",
        "        self.action_size = len(self.action_space)\n",
        "        self.observation_space = env.observation_space\n",
        "        self.observation_shape = env.observation_space.shape\n",
        "        \n",
        "        self.state = None\n",
        "        self.game_name = game_name\n",
        "\n",
        "    def preprocess(self, obs, is_start=False):\n",
        "        obs = self.crop(obs)\n",
        "        grayscale = obs.astype('float32').mean(2)\n",
        "        # s = imresize(grayscale, (self.w, self.h)).astype('float32') * (1.0/255.0)\n",
        "        grayscale = Image.fromarray(grayscale).resize((self.w, self.h))\n",
        "        s = np.array(grayscale).astype('float32') / 255.\n",
        "        s = s.reshape(1, s.shape[0], s.shape[1], 1)\n",
        "        if is_start or self.state is None:\n",
        "            self.state = np.repeat(s, self.num_frames, axis=3)\n",
        "        else:\n",
        "            self.state = np.append(s, self.state[:,:,:,:self.num_frames-1], axis=3)\n",
        "        return self.state\n",
        "\n",
        "#     def render(self):\n",
        "#         self.env.render()\n",
        "\n",
        "    def reset(self):\n",
        "        return self.preprocess(self.env.reset(), is_start=True)\n",
        "\n",
        "    def step(self, action_idx):\n",
        "        action = self.action_space[action_idx]\n",
        "        accum_reward = 0\n",
        "        prev_s = None\n",
        "        for _ in range(self.skip_actions):\n",
        "            s, r, term, info = self.env.step(action)\n",
        "            accum_reward += r\n",
        "            if term:\n",
        "                break\n",
        "            prev_s = s\n",
        "        # Takes maximum value for each pixel value over the current and previous\n",
        "        # frame. Used to get round Atari sprites flickering (Mnih et al. (2015))\n",
        "        if self.game_name == 'SpaceInvaders-v0' and prev_s is not None:\n",
        "            s = np.maximum.reduce([s, prev_s])\n",
        "        return self.preprocess(s), accum_reward, term, info\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMc3OL2_TtBa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "outputId": "14e56df1-f430-4f8e-e537-0a3b15e88671"
      },
      "source": [
        "# env = CustomGym(game_name=\"SpaceInvaders-v0\", crop=lambda img: img[35:-20, :], h=64, w=64)\n",
        "\n",
        "def make_env():\n",
        "    env = gym.make(\"SpaceInvaders-v0\")\n",
        "    env = CustomGym(env, game_name=\"SpaceInvaders-v0\", crop=lambda img: img[35:-20, :], h=64, w=64)\n",
        "    return env\n",
        "\n",
        "env = make_env()\n",
        "\n",
        "obs_shape = env.observation_shape\n",
        "n_actions = env.action_size\n",
        "\n",
        "print(\"Observation shape:\", obs_shape)\n",
        "print(\"Num actions:\", n_actions)\n",
        "print(\"Action names:\", env.env.env.get_action_meanings())\n",
        "print(\"Actions:\", env.action_space)\n",
        "\n",
        "s = env.reset()\n",
        "accum_reward = 0\n",
        "for _ in range(100):\n",
        "    s, reward, _, _ = env.step(random.randint(0,n_actions)-1)\n",
        "    accum_reward += reward\n",
        "\n",
        "plt.title('Game image')\n",
        "plt.imshow(env.render('rgb_array'))\n",
        "plt.show()\n",
        "\n",
        "plt.title('Agent observation (4-frame buffer)')\n",
        "print(s.shape)\n",
        "plt.imshow(s[0].transpose([0,2,1]).reshape([64,-1]))\n",
        "plt.show()"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Observation shape: (210, 160, 3)\n",
            "Num actions: 3\n",
            "Action names: ['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']\n",
            "Actions: [1, 2, 3]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAANEAAAEICAYAAADBfBG8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nk8gs0f8BPhvZOYrvFCQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "(1, 64, 64, 4)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAACDCAYAAACdg+BGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nA9JnXCU7TUWFAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoFPST_CUVqL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Agent():\n",
        "    def __init__(self, session, action_size, optimizer=tf.train.AdamOptimizer(INITIAL_LEARNING_RATE)):\n",
        "\n",
        "        self.action_size = action_size\n",
        "        self.optimizer = optimizer\n",
        "        self.sess = session\n",
        "\n",
        "        with tf.variable_scope('network'):\n",
        "            self.action = tf.placeholder('int32', [None], name='action')\n",
        "            self.target_value = tf.placeholder('float32', [None], name='target_value')\n",
        "\n",
        "            self.state, self.policy, self.value = self.build_model(64, 64, 4)\n",
        "            self.weights = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
        "            scope='network')\n",
        "            self.advantages = tf.placeholder('float32', [None], name='advantages')\n",
        "\n",
        "        with tf.variable_scope('optimizer'):\n",
        "            # Compute the one hot vectors for each action given.\n",
        "            action_one_hot = tf.one_hot(self.action, self.action_size, 1.0, 0.0)\n",
        "\n",
        "            # Clipping required to play nice with logs (0,1)\n",
        "            min_policy = 1e-8\n",
        "            max_policy = 1.0 - 1e-8\n",
        "            self.log_policy = tf.log(tf.clip_by_value(self.policy, 0.000001, 0.999999))\n",
        "\n",
        "            # For a given state and action, compute the log of the policy at\n",
        "            # that action for that state. This also works on batches.\n",
        "            self.log_pi_for_action = tf.reduce_sum(tf.multiply(self.log_policy, action_one_hot), reduction_indices=1)\n",
        "\n",
        "            # Takes in R_t - V(s_t) as in the async paper. Note that we feed in\n",
        "            # the advantages so that V(s_t) is treated as a constant for the\n",
        "            # gradient. This is because V(s_t) is the baseline (called 'b' in\n",
        "            # the REINFORCE algorithm). As long as the baseline is constant wrt\n",
        "            # the parameters we are optimising (in this case those for the\n",
        "            # policy), then the expected value of grad_theta log pi * b is zero,\n",
        "            # so the choice of b doesn't affect the expectation. It reduces the\n",
        "            # variance though.\n",
        "            # We want to do gradient ascent on the expected discounted reward.\n",
        "            # The gradient of the expected discounted reward is the gradient of\n",
        "            # log pi * (R - estimated V), where R is the sampled reward from the\n",
        "            # given state following the policy pi. Since we want to maximise\n",
        "            # this, we define the policy loss as the negative and get tensorflow\n",
        "            # to do the automatic differentiation for us.\n",
        "            self.policy_loss = -tf.reduce_mean(self.log_pi_for_action * self.advantages)\n",
        "\n",
        "            # The value loss is much easier to understand: we want our value\n",
        "            # function to accurately estimated the sampled discounted rewards,\n",
        "            # so we just impose a square error loss.\n",
        "            # Note that the target value should be the discounted reward for the\n",
        "            # state as just sampled.\n",
        "            self.value_loss = tf.reduce_mean(tf.square(self.target_value - self.value))\n",
        "\n",
        "            # We follow Mnih's paper and introduce the entropy as another loss\n",
        "            # to the policy. The entropy of a probability distribution is just\n",
        "            # the expected value of - log P(X), denoted E(-log P(X)), which we\n",
        "            # can compute for our policy at any given state with\n",
        "            # sum(policy * -log(policy)), as below. This will be a positive\n",
        "            # number, since self.policy contains numbers between 0 and 1, so the\n",
        "            # log is negative. Note that entropy is smaller when the probability\n",
        "            # distribution is more concentrated on one action, so a larger\n",
        "            # entropy implies more exploration. Thus we penalise small entropy,\n",
        "            # or equivalently, add -entropy to our loss.\n",
        "            self.entropy = tf.reduce_sum(tf.multiply(self.policy, -self.log_policy))\n",
        "\n",
        "            # Try to minimise the loss. There is some rationale for choosing the\n",
        "            # weighted linear combination here that I found somewhere else that\n",
        "            # I can't remember, but I haven't tried to optimise it.\n",
        "            # Note the negative entropy term, which encourages exploration:\n",
        "            # higher entropy corresponds to less certainty.\n",
        "            self.loss = 0.5 * self.value_loss + self.policy_loss - self.entropy\\\n",
        "            * 0.01\n",
        "\n",
        "            # Compute the gradient of the loss with respect to all the weights,\n",
        "            # and create a list of tuples consisting of the gradient to apply to\n",
        "            # the weight.\n",
        "            grads = tf.gradients(self.loss, self.weights)\n",
        "            grads, _ = tf.clip_by_global_norm(grads, 40.0)\n",
        "            grads_vars = list(zip(grads, self.weights))\n",
        "\n",
        "            # Create an operator to apply the gradients using the optimizer.\n",
        "            # Note that apply_gradients is the second part of minimize() for the\n",
        "            # optimizer, so will minimize the loss.\n",
        "            self.train_op = optimizer.apply_gradients(grads_vars)\n",
        "\n",
        "    def get_policy(self, state):\n",
        "        return self.sess.run(self.policy, {self.state: state}).flatten()\n",
        "\n",
        "    def get_value(self, state):\n",
        "        return self.sess.run(self.value, {self.state: state}).flatten()\n",
        "\n",
        "    def get_policy_and_value(self, state):\n",
        "        policy, value = self.sess.run([self.policy, self.value], {self.state:\n",
        "        state})\n",
        "        return policy.flatten(), value.flatten()\n",
        "\n",
        "    # Train the network on the given states and rewards\n",
        "    def train(self, states, actions, target_values, advantages):\n",
        "        # Training\n",
        "        self.sess.run(self.train_op, feed_dict={\n",
        "            self.state: states,\n",
        "            self.action: actions,\n",
        "            self.target_value: target_values,\n",
        "            self.advantages: advantages\n",
        "        })\n",
        "\n",
        "    # Builds the DQN model as in Mnih, but we get a softmax output for the\n",
        "    # policy from fc1 and a linear output for the value from fc1.\n",
        "    def build_model(self, h, w, channels):\n",
        "        self.layers = {}\n",
        "        state = tf.placeholder('float32', shape=(None, h, w, channels), name='state')\n",
        "        self.layers['state'] = state\n",
        "        # First convolutional layer\n",
        "        with tf.variable_scope('conv1', reuse=tf.AUTO_REUSE):\n",
        "            conv1 = tf.contrib.layers.convolution2d(inputs=state,\n",
        "            num_outputs=16, kernel_size=[8,8], stride=[4,4], padding=\"VALID\",\n",
        "            activation_fn=tf.nn.relu,\n",
        "            weights_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
        "            biases_initializer=tf.zeros_initializer())\n",
        "            self.layers['conv1'] = conv1\n",
        "\n",
        "        # Second convolutional layer\n",
        "        with tf.variable_scope('conv2', reuse=tf.AUTO_REUSE):\n",
        "            conv2 = tf.contrib.layers.convolution2d(inputs=conv1, num_outputs=32,\n",
        "            kernel_size=[4,4], stride=[2,2], padding=\"VALID\",\n",
        "            activation_fn=tf.nn.relu,\n",
        "            weights_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
        "            biases_initializer=tf.zeros_initializer())\n",
        "            self.layers['conv2'] = conv2\n",
        "\n",
        "        # Flatten the network\n",
        "        with tf.variable_scope('flatten', reuse=tf.AUTO_REUSE):\n",
        "            flatten = tf.contrib.layers.flatten(inputs=conv2)\n",
        "            self.layers['flatten'] = flatten\n",
        "\n",
        "        # Fully connected layer with 256 hidden units\n",
        "        with tf.variable_scope('fc1', reuse=tf.AUTO_REUSE):\n",
        "            fc1 = tf.contrib.layers.fully_connected(inputs=flatten, num_outputs=256,\n",
        "            activation_fn=tf.nn.relu,\n",
        "            weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
        "            biases_initializer=tf.zeros_initializer())\n",
        "            self.layers['fc1'] = fc1\n",
        "\n",
        "        # The policy output\n",
        "        with tf.variable_scope('policy', reuse=tf.AUTO_REUSE):\n",
        "            policy = tf.contrib.layers.fully_connected(inputs=fc1,\n",
        "            num_outputs=self.action_size, activation_fn=tf.nn.softmax,\n",
        "            weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
        "            biases_initializer=None)\n",
        "            self.layers['policy'] = policy\n",
        "\n",
        "        # The value output\n",
        "        with tf.variable_scope('value', reuse=tf.AUTO_REUSE):\n",
        "            value = tf.contrib.layers.fully_connected(inputs=fc1, num_outputs=1,\n",
        "            activation_fn=None,\n",
        "            weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
        "            biases_initializer=None)\n",
        "            self.layers['value'] = value\n",
        "\n",
        "        return state, policy, value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2hivJjFaJsr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Summary:\n",
        "    def __init__(self, logdir, agent):\n",
        "        with tf.variable_scope('summary'):\n",
        "            summarising = ['episode_avg_reward', 'avg_value']\n",
        "            self.agent = agent\n",
        "            self.writer = tf.summary.FileWriter(logdir, self.agent.sess.graph)\n",
        "            self.summary_ops = {}\n",
        "            self.summary_vars = {}\n",
        "            self.summary_ph = {}\n",
        "            for s in summarising:\n",
        "                self.summary_vars[s] = tf.Variable(0.0)\n",
        "                self.summary_ph[s] = tf.placeholder('float32', name=s)\n",
        "                self.summary_ops[s] = tf.summary.scalar(s, self.summary_vars[s])\n",
        "            self.update_ops = []\n",
        "            for k in self.summary_vars:\n",
        "                self.update_ops.append(self.summary_vars[k].assign(self.summary_ph[k]))\n",
        "            self.summary_op = tf.summary.merge(list(self.summary_ops.values()))\n",
        "\n",
        "    def write_summary(self, summary, t):\n",
        "        self.agent.sess.run(self.update_ops, {self.summary_ph[k]: v for k, v in summary.items()})\n",
        "        summary_to_add = self.agent.sess.run(self.summary_op, {self.summary_vars[k]: v for k, v in summary.items()})\n",
        "        self.writer.add_summary(summary_to_add, global_step=t)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vg56RbPMb-ZC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def async_trainer(agent, env, sess, thread_idx, T_queue, summary, saver,\n",
        "    save_path):\n",
        "    print (\"Training thread\", thread_idx)\n",
        "    T = T_queue.get()\n",
        "    T_queue.put(T+1)\n",
        "    t = 0\n",
        "\n",
        "    last_verbose = T\n",
        "    last_time = time()\n",
        "    last_target_update = T\n",
        "\n",
        "    terminal = True\n",
        "    while T < T_MAX:\n",
        "        t_start = t\n",
        "        batch_states = []\n",
        "        batch_rewards = []\n",
        "        batch_actions = []\n",
        "        baseline_values = []\n",
        "\n",
        "        if terminal:\n",
        "            terminal = False\n",
        "            state = env.reset()\n",
        "\n",
        "        while not terminal and len(batch_states) < I_ASYNC_UPDATE:\n",
        "            # Save the current state\n",
        "            batch_states.append(state)\n",
        "\n",
        "            # Choose an action randomly according to the policy\n",
        "            # probabilities. We do this anyway to prevent us having to compute\n",
        "            # the baseline value separately.\n",
        "            policy, value = agent.get_policy_and_value(state)\n",
        "            action_idx = np.random.choice(agent.action_size, p=policy)\n",
        "\n",
        "            # Take the action and get the next state, reward and terminal.\n",
        "            state, reward, terminal, _ = env.step(action_idx)\n",
        "\n",
        "            # Update counters\n",
        "            t += 1\n",
        "            T = T_queue.get()\n",
        "            T_queue.put(T+1)\n",
        "\n",
        "            # Clip the reward to be between -1 and 1\n",
        "            reward = np.clip(reward, -1, 1)\n",
        "\n",
        "            # Save the rewards and actions\n",
        "            batch_rewards.append(reward)\n",
        "            batch_actions.append(action_idx)\n",
        "            baseline_values.append(value[0])\n",
        "\n",
        "        target_value = 0\n",
        "        # If the last state was terminal, just put R = 0. Else we want the\n",
        "        # estimated value of the last state.\n",
        "        if not terminal:\n",
        "            target_value = agent.get_value(state)[0]\n",
        "        last_R = target_value\n",
        "\n",
        "        # Compute the sampled n-step discounted reward\n",
        "        batch_target_values = []\n",
        "        for reward in reversed(batch_rewards):\n",
        "            target_value = reward + DISCOUNT_FACTOR * target_value\n",
        "            batch_target_values.append(target_value)\n",
        "        # Reverse the batch target values, so they are in the correct order\n",
        "        # again.\n",
        "        batch_target_values.reverse()\n",
        "\n",
        "        # Test batch targets\n",
        "        if TESTING:\n",
        "            temp_rewards = batch_rewards + [last_R]\n",
        "            test_batch_target_values = []\n",
        "            for j in range(len(batch_rewards)):\n",
        "                test_batch_target_values.append(discount(temp_rewards[j:], DISCOUNT_FACTOR))\n",
        "            if not test_equals(batch_target_values, test_batch_target_values,\n",
        "                1e-5):\n",
        "                print (\"Assertion failed\")\n",
        "                print (last_R)\n",
        "                print (batch_rewards)\n",
        "                print (batch_target_values)\n",
        "                print (test_batch_target_values)\n",
        "\n",
        "        # Compute the estimated value of each state\n",
        "        batch_advantages = np.array(batch_target_values) - np.array(baseline_values)\n",
        "\n",
        "        # Apply asynchronous gradient update\n",
        "        agent.train(np.vstack(batch_states), batch_actions, batch_target_values,\n",
        "        batch_advantages)\n",
        "\n",
        "    global training_finished\n",
        "    training_finished = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvidhkFpcDvD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def estimate_reward(agent, env, episodes=10, max_steps=10000):\n",
        "    episode_rewards = []\n",
        "    episode_vals = []\n",
        "    t = 0\n",
        "    for i in range(episodes):\n",
        "        episode_reward = 0\n",
        "        state = env.reset()\n",
        "        terminal = False\n",
        "        while not terminal:\n",
        "            policy, value = agent.get_policy_and_value(state)\n",
        "            action_idx = np.random.choice(agent.action_size, p=policy)\n",
        "            state, reward, terminal, _ = env.step(action_idx)\n",
        "            t += 1\n",
        "            episode_vals.append(value)\n",
        "            episode_reward += reward\n",
        "            if t > max_steps:\n",
        "                episode_rewards.append(episode_reward)\n",
        "                return episode_rewards, episode_vals\n",
        "        episode_rewards.append(episode_reward)\n",
        "    return episode_rewards, episode_vals"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7RPavlTcHC0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluator(agent, env, sess, T_queue, summary, saver, save_path):\n",
        "    # Read T and put the same T back on.\n",
        "    T = T_queue.get()\n",
        "    T_queue.put(T)\n",
        "    last_time = time()\n",
        "    last_verbose = T\n",
        "    \n",
        "    rewards_history = []\n",
        "    while T < T_MAX:\n",
        "        T = T_queue.get()\n",
        "        T_queue.put(T)\n",
        "        if T - last_verbose >= VERBOSE_EVERY:\n",
        "            print (\"T\", T)\n",
        "            current_time = time()\n",
        "            print (\"Train steps per second\", float(T - last_verbose) / (current_time - last_time))\n",
        "            last_time = current_time\n",
        "            last_verbose = T\n",
        "\n",
        "            print (\"Evaluating agent\")\n",
        "            episode_rewards, episode_vals = estimate_reward(agent, env, episodes=5)\n",
        "            avg_ep_r = np.mean(episode_rewards)\n",
        "            avg_val = np.mean(episode_vals)\n",
        "            print (\"Avg ep reward\", avg_ep_r, \"Average value\", avg_val)\n",
        "            \n",
        "            rewards_history.append(avg_ep_r)\n",
        "            \n",
        "            clear_output(True)\n",
        "            plt.figure(figsize=[8,4])\n",
        "            plt.subplot(1,2,1)\n",
        "            plt.plot(rewards_history, label='rewards')\n",
        "            plt.plot(pd.DataFrame(np.array(rewards_history)).ewm(span=10).mean(), marker='.', label='rewards ewma@10')\n",
        "            plt.title(\"Session rewards\"); plt.grid(); plt.legend()\n",
        "            plt.show()\n",
        "        \n",
        "            global last_T\n",
        "            last_t = T\n",
        "            \n",
        "            summary.write_summary({'episode_avg_reward': avg_ep_r, 'avg_value': avg_val}, T)\n",
        "            checkpoint_file = saver.save(sess, save_path, global_step=T)\n",
        "            print (\"Saved in\", checkpoint_file)\n",
        "        sleep(1.0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SmuIRKgcJtk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def a3c(game_name, num_threads=8, restore=None, save_path='model'):\n",
        "    processes = []\n",
        "    envs = []\n",
        "    \n",
        "    # create num_threads + 1 environments \n",
        "    for _ in range(num_threads+1):\n",
        "        gym_env = gym.make(game_name)\n",
        "        print (\"Assuming ATARI game and playing with pixels\")\n",
        "        env = make_env()\n",
        "        envs.append(env)\n",
        "\n",
        "    # Separate out the evaluation environment\n",
        "    evaluation_env = envs[0]\n",
        "    envs = envs[1:]\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        agent = Agent(session=sess,\n",
        "        action_size=envs[0].action_size,\n",
        "        optimizer=tf.train.AdamOptimizer(INITIAL_LEARNING_RATE))\n",
        "\n",
        "        # Create a saver, and only keep 2 checkpoints.\n",
        "        saver = tf.train.Saver(max_to_keep=2)\n",
        "\n",
        "        T_queue = queue.Queue()\n",
        "\n",
        "        # Either restore the parameters or don't.\n",
        "        if restore is not None:\n",
        "            saver.restore(sess, save_path + '-' + str(restore))\n",
        "            last_T = restore\n",
        "            print (\"T was:\", last_T)\n",
        "            T_queue.put(last_T)\n",
        "        else:\n",
        "            sess.run(tf.global_variables_initializer())\n",
        "            T_queue.put(0)\n",
        "\n",
        "        summary = Summary(LOG_DIR, agent)\n",
        "\n",
        "        # Create a process for each worker\n",
        "        for i in range(num_threads):\n",
        "            processes.append(threading.Thread(target=async_trainer, args=(agent,\n",
        "            envs[i], sess, i, T_queue, summary, saver, save_path,)))\n",
        "\n",
        "        # Create a process to evaluate the agent\n",
        "        processes.append(threading.Thread(target=evaluator, args=(agent,\n",
        "        evaluation_env, sess, T_queue, summary, saver, save_path,)))\n",
        "\n",
        "        # Start all the processes\n",
        "        for p in processes:\n",
        "            p.daemon = True\n",
        "            p.start()\n",
        "\n",
        "        # Until training is finished\n",
        "        while not training_finished:\n",
        "            sleep(0.01)\n",
        "\n",
        "        # Join the processes, so we get this thread back.\n",
        "        for p in processes:\n",
        "            p.join()\n",
        "\n",
        "# Returns sum(rewards[i] * gamma**i)\n",
        "def discount(rewards, gamma):\n",
        "    return np.sum([rewards[i] * gamma**i for i in range(len(rewards))])\n",
        "\n",
        "def test_equals(arr1, arr2, eps):\n",
        "    return np.sum(np.abs(np.array(arr1)-np.array(arr2))) < eps"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiXwzmqTfFnp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "edd4d921-cc79-4ccc-995e-5a91e9d6e372"
      },
      "source": [
        "model_id = model_id = str(datetime.date.today()) + \"-\" + str(uuid.uuid1())\n",
        "print(model_id)"
      ],
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-05-03-5cf8c1c8-6df4-11e9-ae34-0242ac1c0002\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lKgoMWxDcSBV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4057
        },
        "outputId": "5dca200f-cfa6-410c-e2a6-b9694010989e"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "\n",
        "save_path = F\"/content/gdrive/My Drive/RL_models/\" + model_id\n",
        "restore = None\n",
        "game_name = \"SpaceInvaders-v0\"\n",
        "\n",
        "reward_history = []\n",
        "\n",
        "a3c(game_name, num_threads=NUM_THREADS, restore=None,\n",
        "    save_path=save_path)"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Assuming ATARI game and playing with pixels\n",
            "Assuming ATARI game and playing with pixels\n",
            "Assuming ATARI game and playing with pixels\n",
            "Assuming ATARI game and playing with pixels\n",
            "Assuming ATARI game and playing with pixels\n",
            "Assuming ATARI game and playing with pixels\n",
            "Assuming ATARI game and playing with pixels\n",
            "Assuming ATARI game and playing with pixels\n",
            "Assuming ATARI game and playing with pixels\n",
            "Training thread 0Training thread\n",
            " 1\n",
            "Training thread 2\n",
            "Training thread 3\n",
            "Training thread 4\n",
            "Training thread 5\n",
            "Training thread 6\n",
            "Training thread 7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception in thread Thread-58:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1334, in _do_call\n",
            "    return fn(*args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1319, in _run_fn\n",
            "    options, feed_dict, fetch_list, target_list, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1407, in _call_tf_sessionrun\n",
            "    run_metadata)\n",
            "tensorflow.python.framework.errors_impl.CancelledError: Run call was cancelled\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 864, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"<ipython-input-118-82449cca8adc>\", line 85, in async_trainer\n",
            "    batch_advantages)\n",
            "  File \"<ipython-input-116-3d4e7948bc6f>\", line 102, in train\n",
            "    self.advantages: advantages\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 929, in run\n",
            "    run_metadata_ptr)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1152, in _run\n",
            "    feed_dict_tensor, options, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1328, in _do_run\n",
            "    run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1348, in _do_call\n",
            "    raise type(e)(node_def, op, message)\n",
            "tensorflow.python.framework.errors_impl.CancelledError: Run call was cancelled\n",
            "\n",
            "Exception in thread Thread-60:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1334, in _do_call\n",
            "    return fn(*args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1319, in _run_fn\n",
            "    options, feed_dict, fetch_list, target_list, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1407, in _call_tf_sessionrun\n",
            "    run_metadata)\n",
            "tensorflow.python.framework.errors_impl.CancelledError: Run call was cancelled\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 864, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"<ipython-input-118-82449cca8adc>\", line 31, in async_trainer\n",
            "    policy, value = agent.get_policy_and_value(state)\n",
            "  File \"<ipython-input-116-3d4e7948bc6f>\", line 92, in get_policy_and_value\n",
            "    state})\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 929, in run\n",
            "    run_metadata_ptr)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1152, in _run\n",
            "    feed_dict_tensor, options, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1328, in _do_run\n",
            "    run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1348, in _do_call\n",
            "    raise type(e)(node_def, op, message)\n",
            "tensorflow.python.framework.errors_impl.CancelledError: Run call was cancelled\n",
            "\n",
            "Exception in thread Thread-61:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1334, in _do_call\n",
            "    return fn(*args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1319, in _run_fn\n",
            "    options, feed_dict, fetch_list, target_list, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1407, in _call_tf_sessionrun\n",
            "    run_metadata)\n",
            "tensorflow.python.framework.errors_impl.CancelledError: Run call was cancelled\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 864, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"<ipython-input-118-82449cca8adc>\", line 85, in async_trainer\n",
            "    batch_advantages)\n",
            "  File \"<ipython-input-116-3d4e7948bc6f>\", line 102, in train\n",
            "    self.advantages: advantages\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 929, in run\n",
            "    run_metadata_ptr)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1152, in _run\n",
            "    feed_dict_tensor, options, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1328, in _do_run\n",
            "    run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1348, in _do_call\n",
            "    raise type(e)(node_def, op, message)\n",
            "tensorflow.python.framework.errors_impl.CancelledError: Run call was cancelled\n",
            "\n",
            "Exception in thread Thread-55:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1334, in _do_call\n",
            "    return fn(*args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1319, in _run_fn\n",
            "    options, feed_dict, fetch_list, target_list, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1407, in _call_tf_sessionrun\n",
            "    run_metadata)\n",
            "tensorflow.python.framework.errors_impl.CancelledError: Run call was cancelled\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 864, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"<ipython-input-118-82449cca8adc>\", line 85, in async_trainer\n",
            "    batch_advantages)\n",
            "  File \"<ipython-input-116-3d4e7948bc6f>\", line 102, in train\n",
            "    self.advantages: advantages\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 929, in run\n",
            "    run_metadata_ptr)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1152, in _run\n",
            "    feed_dict_tensor, options, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1328, in _do_run\n",
            "    run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1348, in _do_call\n",
            "    raise type(e)(node_def, op, message)\n",
            "tensorflow.python.framework.errors_impl.CancelledError: Run call was cancelled\n",
            "\n",
            "Exception in thread Thread-62:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 864, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"<ipython-input-118-82449cca8adc>\", line 31, in async_trainer\n",
            "    policy, value = agent.get_policy_and_value(state)\n",
            "  File \"<ipython-input-116-3d4e7948bc6f>\", line 92, in get_policy_and_value\n",
            "    state})\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 929, in run\n",
            "    run_metadata_ptr)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1075, in _run\n",
            "    raise RuntimeError('Attempted to use a closed Session.')\n",
            "RuntimeError: Attempted to use a closed Session.\n",
            "Exception in thread Thread-59:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1334, in _do_call\n",
            "    return fn(*args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1319, in _run_fn\n",
            "    options, feed_dict, fetch_list, target_list, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1407, in _call_tf_sessionrun\n",
            "    run_metadata)\n",
            "tensorflow.python.framework.errors_impl.CancelledError: Run call was cancelled\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 864, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"<ipython-input-118-82449cca8adc>\", line 85, in async_trainer\n",
            "    batch_advantages)\n",
            "  File \"<ipython-input-116-3d4e7948bc6f>\", line 102, in train\n",
            "    self.advantages: advantages\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 929, in run\n",
            "    run_metadata_ptr)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1152, in _run\n",
            "    feed_dict_tensor, options, run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1328, in _do_run\n",
            "    run_metadata)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1348, in _do_call\n",
            "    raise type(e)(node_def, op, message)\n",
            "tensorflow.python.framework.errors_impl.CancelledError: Run call was cancelled\n",
            "\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-133-83209dbbcce2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m a3c(game_name, num_threads=NUM_THREADS, restore=None,\n\u001b[0;32m---> 10\u001b[0;31m     save_path=save_path)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-124-f8f32c91cfb1>\u001b[0m in \u001b[0;36ma3c\u001b[0;34m(game_name, num_threads, restore, save_path)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m# Until training is finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtraining_finished\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m             \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# Join the processes, so we get this thread back.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "stream",
          "text": [
            "Exception in thread Thread-56:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 864, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"<ipython-input-118-82449cca8adc>\", line 31, in async_trainer\n",
            "    policy, value = agent.get_policy_and_value(state)\n",
            "  File \"<ipython-input-116-3d4e7948bc6f>\", line 92, in get_policy_and_value\n",
            "    state})\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 929, in run\n",
            "    run_metadata_ptr)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1075, in _run\n",
            "    raise RuntimeError('Attempted to use a closed Session.')\n",
            "RuntimeError: Attempted to use a closed Session.\n",
            "\n",
            "Exception in thread Thread-57:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/threading.py\", line 864, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"<ipython-input-118-82449cca8adc>\", line 31, in async_trainer\n",
            "    policy, value = agent.get_policy_and_value(state)\n",
            "  File \"<ipython-input-116-3d4e7948bc6f>\", line 92, in get_policy_and_value\n",
            "    state})\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 929, in run\n",
            "    run_metadata_ptr)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1075, in _run\n",
            "    raise RuntimeError('Attempted to use a closed Session.')\n",
            "RuntimeError: Attempted to use a closed Session.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvWiWrgYceCz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def play(agent, game_name, num_episodes=10, fps=5.0, monitor=True):\n",
        "#     gym_env = gym.make(game_name)\n",
        "#     if monitor:\n",
        "#         print(gym_env)\n",
        "#         gym_env = gym.wrappers.Monitor(gym_env, directory=\"SI_videos\", video_callable=lambda episode_id: True, force=True)\n",
        "#     print (gym_env)\n",
        "#     print(game_name)\n",
        "    env = make_env()\n",
        "    env = gym.wrappers.Monitor(env, directory=\"SI_videos\", video_callable=lambda episode_id: True, force=True)\n",
        "\n",
        "    desired_frame_length = 1.0 / fps\n",
        "\n",
        "    episode_rewards = []\n",
        "    episode_vals = []\n",
        "    t = 0\n",
        "    for ep in range(num_episodes):\n",
        "        print (\"Starting episode\", ep)\n",
        "        episode_reward = 0\n",
        "        state = env.reset()\n",
        "        print (state.shape)\n",
        "        terminal = False\n",
        "        current_time = time()\n",
        "        while not terminal:\n",
        "            policy, value = agent.get_policy_and_value(state)\n",
        "            action_idx = np.random.choice(agent.action_size, p=policy)\n",
        "            state, reward, terminal, _ = env.step(action_idx)\n",
        "            t += 1\n",
        "            episode_vals.append(value)\n",
        "            episode_reward += reward\n",
        "            # Sleep so the frame rate is correct\n",
        "            next_time = time()\n",
        "            frame_length = next_time - current_time\n",
        "            if frame_length < desired_frame_length:\n",
        "                sleep(desired_frame_length - frame_length)\n",
        "            current_time = next_time\n",
        "        episode_rewards.append(episode_reward)\n",
        "    return episode_rewards, episode_vals\n",
        "\n",
        "def run_agent(save_path, T, game_name):\n",
        "    with tf.Session() as sess:\n",
        "        agent = Agent(session=sess,\n",
        "        action_size=3,\n",
        "        optimizer=tf.train.AdamOptimizer(1e-4))\n",
        "\n",
        "        # Create a saver, and restore checkpoint.\n",
        "        saver = tf.train.Saver()\n",
        "        saver.restore(sess, save_path + '-' + str(T))\n",
        "\n",
        "        episode_rewards, episode_vals = play(agent, game_name, num_episodes=2)\n",
        "\n",
        "        return sess, agent, episode_rewards, episode_vals"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CTKCW7NhZIn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "d83b50a7-552b-4ce2-af1b-fc0acc3a9df1"
      },
      "source": [
        "tf.reset_default_graph()\n",
        "# call to run the agent \n",
        "print (last_T)\n",
        "s, a, e_r, e_v = run_agent(F\"/content/gdrive/My Drive/RL_models/\" + \"2019-05-03-506b9b98-6de4-11e9-ae34-0242ac1c0002\", 280797, game_name)\n",
        "print (e_r)"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n",
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/RL_models/2019-05-03-506b9b98-6de4-11e9-ae34-0242ac1c0002-280797\n",
            "Starting episode 0\n",
            "(1, 64, 64, 4)\n",
            "Starting episode 1\n",
            "(1, 64, 64, 4)\n",
            "[110.0, 50.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8d0dRJKs6EO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e88863be-f162-4611-88c2-645180a289d0"
      },
      "source": [
        "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./SI_videos/\")))\n",
        "print(video_names)"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['openaigym.video.15.14956.video000001.mp4', 'openaigym.video.15.14956.video000000.mp4']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "108V3ApTCVSF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download('./SI_videos/openaigym.video.15.14956.video000001.mp4')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4EgDj7zBZyj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}